---
title: "DACSS603Final"
author: "Theresa Szczepanski"
desription: "MCAS G9 Science Analysis"
date: "10/22/2023"

format:
  html:
    df-print: paged
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
    
bibliography: references.bib

editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: setup
#| warning: false
#| message: false

source('dependencies.R')
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

# Research Questions

The Massachusetts Education Reform Act in 1993 was passed in the context
of a national movement toward education reform throughout the United
States. As early as 1989 there were calls to establish national
curriculum standards as a way to improve student college and career
readiness skills and close poverty gaps [@Greer18]. Massachusetts
Comprehensive Assessment System (MCAS) tests were introduced as part of
the Massachusetts Education Reform Act.

The MCAS tests are a significant tool for educational equity. Scores on
the Grade 10 Math MCAS test "predict longer-term educational attainments
and labor market success, above and beyond typical markers of student
advantage. For example, among demographically similar students who
attended the same high school and have the same level of ultimate
educational attainment, those with higher MCAS mathematics scores go on
to have much higher average earnings than those with lower scores."
[@Boats20].

With the introduction of the new Common Core standards came the demand
for appropriate curricular materials and teaching practices. Research
indicates that the choice of instructional materials can have an impact
"as large as or larger than the impact of teacher quality" [@Blindly12].
Massachusetts, along with Arkansas, Delaware, Kentucky, Louisiana,
Maryland, Mississippi, Nebraska, New Mexico, Ohio, Rhode Island,
Tennessee and Texas belongs to the Council of Chief State School
Officers' (CCSO), [High Quality Instructional Materials and Professional
Development
network](https://learning.ccsso.org/high-quality-instructional-materials)
which aims to close the "opportunity gap" amongst students by ensuring that 
every teacher has access to high-quality, standards aligned
instructional materials and receives relevant professional development
to support their use of these materials [@IMPD21].

All Massachusetts Public School students must complete a High School science MCAS
exam providing a wealth of standardized data on student's content understanding.
All school's receive annual summary reports on student performance as well as 
disaggregated student item performance reports [@MCASIT]. Significant work has been 
done using MCAS data and the Student Opportunity Act to address funding inequities
across the Commonwealth [@Boats20]. With funding gaps outlined in the late 1990's 
closing, one could consider how the MCAS data could be leveraged to support the state's 
current high quality instructional materials initiatives by providing schools with guidance on curricular areas in most need of support.

Using the Curricular information provided in state wide Next Generation MCAS High School Introductory Physics
student performance data, we hope to address the following broad
questions:

```{=html}
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
```
::: blue
-   Is there a relationship between differences in student performance
    across Science Practice Categories and student achievement?

-   How can trends in student performance be used to identify
    Introductory Physics content areas in need of curricular
    adjustments?
:::


In this report, I will analyze the High School Introductory Physics Next
Generation [Massachusetts Comprehensive Assessment System
(MCAS)](https://www.doe.mass.edu/mcas/default.html) tests results for
Massachusetts public schools.

Data for the study were drawn from DESEâ€™s Next Generation MCAS Test
[Achievement Results statewide report](https://profiles.doe.mass.edu/statereport/mcas.aspx),
[Item Analysis statewide report](https://profiles.doe.mass.edu/statereport/nextgenmcas_item.aspx), and 
the [MCAS digital item library](https://mcas.digitalitemlibrary.com/home?subject=Science&grades=Physics&view=ALL). Throughout the commonwealth, 27,745 students from 112 schools participated in the 
2022 and 2023 Next Generation HighSchool Introductory Physics MCAS. 
The assessment consists of 42 multiple choice and constructed response items that 
assess students on Physical Science standards from the [2016 STE Massachusetts
Curriculum Framework](https://www.doe.mass.edu/frameworks/scitech/2016-04.pdf) 
in the `Reporting Categories` of Motions and Forces, `MF`, 
Energy, `EN`, and Waves, `WA`. Each item is associated with a specific content standard from the Massachusetts Curriculum Framework as well as an underlying science `Practice Category` of 
Evidence Reasoning and Modeling, `ERM`, Mathematics and Data, `MD`, or Investigations 
and Questioning, `IQ`. The State Item Report provides the percentage of points earned by 
students in a school for each item as well as the percentage of points earned by 
all students in the state for each item.


The `HSPhy_NextGen_SchoolSum` data frame contains performance results and 
demographic characteristics from
112 public schools across the commonwealth on the Next Generation High
School Introductory Physics MCAS, which was administered in the Spring
of 2022 and 2023. 87 of those schools
tested students in both years and 25 of those schools only tested
students in 1 of the 2 testing years.

For each school, there are values reported for 44 different variables
which consist of information from three broad categories

-   *School Characteristics*: This includes the name of the school and
    the size of the school, `School Size`, as determined by the number of students that
    completed the MCAS exam.

-   *Science Practice and Reporting Category Performance Metrics*: This
    includes the percentage of points earned by students at a school in 
    each content category, `MF%`, `EN%`, `WA%` and practice category 
    `ERM%`, `MD%`, `IQ%`, the difference between a school's percentage of points 
    earned compared to student's in the state (`MFDiff`, `ENDiff`, etc...), and 
    the variability in a school's performance relative to the state by category as 
    measured by the standard deviation (`SD MF Diff`, `SD EN Diff`, etc...).
    

-   *Aggregate Performance Level metrics*: This includes a school's percentage
    of students at each of the four `Performance Levels`, (`E%`: Exceeding
    Expectations, `M%`: Meeting Expectations, `PM%`: Partially Meeting
    Expectations, and `NM%`: Not Meeting Expectations), as well as a
    the difference between these percentages and the percentage of students 
    in Massachusetts at each performance level (`EDiff`, `MDiff`, `PMDiff`, `NMDiff`).



See the `HSPhy_NextGenMCASDF` data frame summary and **codebook** for
further details about all variables.

# Hypothesis

```{=html}
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
```
::: blue
-   A school's percent of student's exceeding expectations on the
    Introductory Physics MCAS is negatively associated with variance in
    Mathematics and Data items.

-   School's mean performance diffs are not the same across science
    practice categories.
:::



# Descriptive Statistics

```{r}
#| label: dataframe setup
#| warning: false
#| message: false

#HSPhy_NextGen_SchoolSum
HSPhy_NextGen_SchoolSum<-HSPhy_NextGen_SchoolSum%>%
  ungroup()

#HSPhy_NextGen_SchoolSum
# HSPhy_NextGen_PerfDF
# HSPhy_NextGen_SchoolIT301DF

HSPhy_2023_SchoolSizeDF<-read_excel("data/2023_Physics_NextGenMCASItem.xlsx", skip = 1)%>%
  select(`School Name`, `School Code`, `Tested`)%>%
  mutate(`Tested` = as.integer(`Tested`))%>%
  select(`School Name`, `School Code`, `Tested`)

HSPhy_2022_SchoolSizeDF<-read_excel("data/2022_Physics_NextGenMCASItem.xlsx", skip = 1)%>%
  select(`School Name`, `School Code`, `Tested`)%>%
  mutate(`Tested` = as.integer(`Tested`))%>%
  select(`School Name`, `School Code`, `Tested`)


HSPhy_SchoolSize <- rbind(HSPhy_2023_SchoolSizeDF, HSPhy_2022_SchoolSizeDF)%>%
  mutate(count = 1)%>%
  group_by(`School Name`, `School Code`)%>%
  summarise(count = sum(count),
            `Tested` = sum(`Tested`))%>%
  mutate(`Tested Count` = round(`Tested`/count))%>%
  ungroup()
#HSPhy_SchoolSize
quantile <- quantile(HSPhy_SchoolSize$`Tested Count`)
HSPhy_Size<-HSPhy_SchoolSize%>%
  mutate(`School Size` = case_when(
    `Tested Count` <= quantile[2] ~ "Small",
    `Tested Count` > quantile[2] &
      `Tested Count` <= quantile[3] ~ "Low-Mid",
    `Tested Count` > quantile[3] &
      `Tested Count` <= quantile[4] ~ "Upper-Mid",
    `Tested Count` > quantile[4] &
      `Tested Count` <= quantile[5] ~ "Large",
  ))%>%
  mutate(`School Size` = recode_factor(`School Size`,
                                            "Small" = "Small",
                                            "Low-Mid" = "Low-Mid",
                                            "Upper-Mid" = "Upper-Mid",
                                            "Large" = "Large",
                                            .ordered = TRUE))%>%
  select(`School Name`, `School Code`, `School Size`)


#HSPhy_Size

HSPhy_NextGen_SchoolSum<-HSPhy_NextGen_SchoolSum%>%
  left_join(HSPhy_Size, by = c("School Name" = "School Name", "School Code" = "School Code"))%>%
  mutate(`EMDiff` = `EDiff` + `MDiff`)%>%
  mutate(`EM Perf Stat` = case_when(
    `EDiff` > 0 & `EDiff` + `MDiff` > 0 ~ "HighEM",
    `EDiff` <= 0 & `EDiff` + `MDiff` > 0 ~ "HighM",
    #`EMDiff` > quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)[3] & 
      `EMDiff` <= 0  & `EMDiff` > -14 ~ "Mid",
    `EMDiff` <= -14 & `EMDiff` >= -33  ~ "Mid-Low",
    `EMDiff` < -33  ~ "Low"
   
  ))%>%
  mutate(`EM Perf Stat` = recode_factor(`EM Perf Stat`,
                                 "HighEM" = "HighEM",
                                 "HighM" = "HighM",
                                 "Mid" = "Mid",
                                 "Mid-Low" = "Mid-Low",
                                 "Low" = "Low",
                                 .ordered = TRUE))
HSPhy_NextGen_SchoolSum
#quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)

                                      

#summary(HSPhy_NextGen_SchoolSum)
print(summarytools::dfSummary(HSPhy_NextGen_SchoolSum,
                         varnumbers = FALSE,
                         plain.ascii  = FALSE,
                         style        = "grid",
                         graph.magnif = 0.70,
                        valid.col    = FALSE),
       method = 'render',
       table.classes = 'table-condensed')

```

## Key Variables

To explore the relationship between the distribution of Student `Performance Level`
and student's performance in content categories, we examine the percentage of points 
earned by students at school's as well as the standard deviation of the difference between points earned by students at a school and points earned by students in the state across `Reporting Categories` and `Practice Categories`. We grouped schools by their `EM Perf Stat`, an ordinal variable classifying schools 
by the percentage of students they have that were classified as either Exceeding or 
Meeting expectations on the MCAS. These numbers seem to suggest that items classified 
with the Scientific Practice of `Mathematics and Data` seem to be more challenging to students than those 
classified as `Evidence, Reasoning, and Modeling`. When considering content Reporting Categories,
there do not seem to be discernible distinctions between student achievement and performance 
across categories, except that school's with the weakest achievement, seem to have more success 
on items from the `Waves` reporting category.

```{r}

 #quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)


 
HSPhy_NextGen_SchoolSum%>%
  group_by(`EM Perf Stat`)%>%
    summarise( `Mean MD%` = mean(`MD%`), 
              `Mean MD SD` = mean(`MD Diff SD`),
              `Mean ERM%` = mean(`ERM%`),
               `Mean ERM SD` = mean (`ERM Diff SD`))


HSPhy_NextGen_SchoolSum%>%
  group_by(`EM Perf Stat`)%>%
    summarise( `Mean MF%` = mean(`MF%`), 
              `Mean MF SD` = mean(`MF Diff SD`),
              `Mean EN%` = mean(`EN%`),
               `Mean EN SD` = mean (`EN Diff SD`),
              `Mean WA%` = mean(`WA%`),
               `Mean WA SD` = mean (`WA Diff SD`)
              )



```

# Visualization

## Distribution of Performance Level %

When examining the statewide performance distribution, we can see from the right-skew 
that it is rare for schools to have high percentages of students Not Meeting expectations
and even rarer for schools to have high percentages of students Exceeding expectations.

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`E%`, `M%`, `PM%`, `NM%`)%>%
  pivot_longer(c(1:4), names_to = "Performance Level", values_to = "% Students")%>%
   ggplot( aes(x=`% Students`, color=`Performance Level`, fill=`Performance Level`)) +
    geom_histogram(alpha=0.6, binwidth = 15) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Performance Level`)+
      labs( y = "",
            title = "School Performance Level Distribution",
            x = "% Students at Performance Level",
            caption = "NextGen HS Physics MCAS")


```

## Distribution of School Performance and Variability by Practice Cat

Although `Mathematics and Data` and `Evidence, Reasoning, and Modeling` items 
have equal weighting in the HS Introductory Physics exam, student performance distributions
are noticeably different across these practice categories.

```{r}

HSPhy_NextGen_SchoolSum%>%
  select(`ERM%`, `MD%`)%>%
  pivot_longer(c(1:2), names_to = "Practice Cat", values_to = "% Points")%>%
   ggplot( aes(x=`% Points`, color=`Practice Cat`, fill=`Practice Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Practice Cat`)+
      labs( y = "% Points Earned",
            title = "School Performance by Practice Category",
            x = "",
            caption = "NextGen HS Physics MCAS")
  #ggtitle("Practice Category Performance")


```


When considering the variability of a school's performance on items relative 
to the state by Practice Category, `SD MD Diff`, and `SD ERM Diff`, we can 
see that `Mathematics and Data` is skewed more to the right.

```{r}
  
  HSPhy_NextGen_SchoolSum%>%
  select(`ERM Diff SD`, `MD Diff SD`)%>%
  pivot_longer(c(1:2), names_to = "Practice Cat", values_to = "SD Diff")%>%
   ggplot( aes(x=`SD Diff`, color=`Practice Cat`, fill=`Practice Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
   # theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
      labs( y = "SD Diff",
            title = "School Performance Variation by Practice Category",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    facet_wrap(~`Practice Cat`)
```
## Mathematics and Data vs. Evidence Reasoning and Modeling (Practice Category)

These images, seem to suggest that schools with the highest percentage of students Exceeding 
expectations on the MCAS have the lowest levels of variation in performance on 
Mathematics and Data Items and schools with the lowest percentage of students 
Exceeding expectations on the MCAS have the highest levels of variation in performance
on Mathematics and Data Items.

```{r}


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM Diff SD`, `MD Diff SD` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`SD Diff`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    
    theme(
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      #axis.text.x=element_blank()
    ) +
 
    labs( y = "SD Diff",
            title = "Student Performance Variation by Practice Category",
            x = "",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Practice Cat`)


      


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM Diff SD`, `MD Diff SD` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `Practice Cat`, y=`SD Diff`, fill= `Practice Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
     
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "SD Diff",
            title = "Student Practice Cat. Variation by Achievement Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`)
```
These images, seem to suggest that students at all schools seem to have 
more difficulty with Mathematics and Data items as compared to Evidence, Reasoning, 
and Modeling Items.

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM%`, `MD%` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`%Points`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
      
      plot.title = element_text(size=11)
    ) +
    labs( y = "%Points Earned",
            title = "Student Practice Cat. Achievement by Performance Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`Practice Cat`)
```

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM%`, `MD%` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
  ggplot( aes(x= `Practice Cat`, y=`%Points`, fill= `Practice Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
      
      plot.title = element_text(size=11)
    ) +
    labs( y = "%Points Earned",
            title = "Student Practice Cat. Achievement by Performance Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`, scale ="free_y")
# HSPhy_NextGen_SchoolSum%>%
#   select(`EM Perf Stat`, `ERMDiff`, `MDDiff` )%>%
#   pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
#   ggplot( aes(x= `EM Perf Stat`, y=`%Points`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     #theme_ipsum() +
#     theme(
#       
#       plot.title = element_text(size=11)
#     ) +
#     labs( y = "%Points Earned",
#             title = "Student Practice Cat. Achievement by Performance Level",
#             x = "",
#             caption = "NextGen HS Physics MCAS") +
#     #xlab("")+
#   facet_wrap(~`Practice Cat`)

# HSPhy_NextGen_SchoolSum%>%
#   select(`EM Perf Stat`, `ERM%`, `MD%` )%>%
#   pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
#   ggplot( aes(x= `Practice Cat`, y=`%Points`, fill= `Practice Cat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     #theme_ipsum() +
#     theme(
#       
#       plot.title = element_text(size=11)
#     ) +
#     labs( y = "%Points Earned",
#             title = "Student Practice Cat. Achievement by Performance Level",
#             x = "",
#             caption = "NextGen HS Physics MCAS") +
#     #xlab("")+
#   facet_wrap(~`EM Perf Stat`, scale ="free_y")
  

# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`MD%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")

# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`MD Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`ERM Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`ERM%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")


```



## Distribution of School Performance and Variability by Reporting Cat

Here we can visualize the variability of a school's performance on items partitioned 
instead by Content Reporting Category, `MF%`/`SD MF Diff`,  `EN%`/`SD EN Diff`, and 
`WA%`/`SD WA Diff`.


```{r}
  HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x=`SD Diff`, color=`Report Cat`, fill=`Report Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
      labs( y = "SD Diff",
            title = "School Performance Variation by Content Reporting Category",
            x = "",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Report Cat`)

```

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF%`, `EN%`, `WA%` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "% Points")%>%
 ggplot( aes(x=`% Points`, color=`Report Cat`, fill=`Report Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Report Cat`)+
      labs( y = "% Points Earned",
            title = "Student Performance by Content Reporting Category",
            x = "",
            caption = "NextGen HS Physics MCAS")
  #ggtitle("Practice Category Performance")


```



## Motion and Forces vs. Energy vs. Waves (Reporting Category)

These images suggest that most schools exhibit similar levels of variability 
in performance relative to the state across all reporting categories. Schools with the lowest percentage of students Exceeding expectations exhibit high variability in performance across all content reporting 
categories, but seem to have lower variability on `Waves` items.


```{r}

HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`SD Diff`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    
    theme(
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
 
    labs( y = "SD Diff",
            title = "School Performance Variation by Content Reporting Category",
            x = "",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Report Cat`)


      


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `Report Cat`, y=`SD Diff`, fill= `Report Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
     
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "SD Diff",
            title = "School Content Reporting Cat. Variation by Achievement Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`)


# HSPhy_NextGen_SchoolSum%>%
#   select(`EM Perf Stat`, `MF%`, `EN%`, `WA%` )%>%
#   pivot_longer(c(2:4), names_to = "Report Cat", values_to = "% Points")%>%
#   ggplot( aes(x= `Report Cat`, y=`% Points`, fill= `Report Cat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     #theme_ipsum() +
#     theme(
#      
#       plot.title = element_text(size=11),
#       axis.title.x=element_blank(),
#       axis.text.x=element_blank()
#     ) +
#      labs( y = "SD Diff",
#             title = "School Content Reporting Cat. Performance by Achievement Level",
#             x = "",
#             caption = "NextGen HS Physics MCAS") +
#     #xlab("")+
#   facet_wrap(~`EM Perf Stat`)

# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`MF%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`MF Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# 
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`EN%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`EN Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`WA%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`WA Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")

```

# References
