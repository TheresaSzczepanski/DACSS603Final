---
title: "DACSS603Final"
author: "Theresa Szczepanski"
desription: "MCAS G9 Science Analysis"
date: "10/22/2023"

format:
  html:
    embed-resources: true
    self-contained-math: true
    df-print: paged
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
    
bibliography: references.bib

editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: setup
#| warning: false
#| message: false

source('dependencies.R')
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
#install.packages("stargazer")
#library(stargazer)
#library(flexmix)

#library(qpcR)
```

# Research Questions

The Massachusetts Education Reform Act in 1993 was passed in the context
of a national movement toward education reform throughout the United
States. As early as 1989 there were calls to establish national
curriculum standards as a way to improve student college and career
readiness skills and close poverty gaps [@Greer18]. Massachusetts
Comprehensive Assessment System (MCAS) tests were introduced as part of
the Massachusetts Education Reform Act.

The MCAS tests are a significant tool for educational equity. Scores on
the Grade 10 Math MCAS test "predict longer-term educational attainments
and labor market success, above and beyond typical markers of student
advantage" and differences among students are largely and "sometimes
completely accounted for" by differences in 10th grade MCAS scores and
educational attainments. [@Boats20].

With the introduction of the new Common Core standards and
accountability testing came the demand for aligned curricular materials
and teaching practices. Research indicates that the choice of
instructional materials can have an impact "as large as or larger than
the impact of teacher quality" [@Blindly12]. Massachusetts, along with
Arkansas, Delaware, Kentucky, Louisiana, Maryland, Mississippi,
Nebraska, New Mexico, Ohio, Rhode Island, Tennessee, and Texas belongs
to the Council of Chief State School Officers' (CCSO), [High Quality
Instructional Materials and Professional Development
network](https://learning.ccsso.org/high-quality-instructional-materials)
which aims to close the "opportunity gap" among students by ensuring
that every teacher has access to high-quality, standards aligned
instructional materials and receives relevant professional development
to support their use of these materials [@IMPD21].

All Massachusetts Public School students must complete a High School
science MCAS exam providing a wealth of standardized data on students'
discipline specific skill development. All schools receive annual
summary reports on student performance. Significant work has been done
using the MCAS achievement data and the Student Opportunity Act to
identify achievement gaps and address funding inequities across the
Commonwealth [@Boats20]. With funding gaps outlined in the late 1990's
closing, one could consider how the MCAS data could be leveraged to
support the state's current high quality instructional materials
initiatives. The state compiles school's performance disaggregated by
each MCAS question item [@MCASIT].

Using the curricular information provided in state wide Next Generation
MCAS High School Introductory Physics Item reports together with
school-level student performance data, we hope to address the following
broad questions:

```{=html}
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
```
::: blue
-   Is there a relationship between differences in a school's
    performance across Science Practice Categories and a school's
    overall achievement on the Introductory Physics exam?

-   How can trends in a school's performance be used to provide schools
    with guidance on discipline-specific curricular areas to target to
    improve student achievement?
:::

In this report, I will analyze the High School Introductory Physics Next
Generation [Massachusetts Comprehensive Assessment System
(MCAS)](https://www.doe.mass.edu/mcas/default.html) tests results for
Massachusetts public schools.

Data for the study were drawn from DESE's Next Generation MCAS Test
[Achievement Results statewide
report](https://profiles.doe.mass.edu/statereport/mcas.aspx), [Item
Analysis statewide
report](https://profiles.doe.mass.edu/statereport/nextgenmcas_item.aspx),
and the [MCAS digital item
library](https://mcas.digitalitemlibrary.com/home?subject=Science&grades=Physics&view=ALL).
The Next Generation High School Introductory Physics MCAS assessment
consists of 42 multiple choice and constructed response items that
assess students on Physical Science standards from the [2016 STE
Massachusetts Curriculum
Framework](https://www.doe.mass.edu/frameworks/scitech/2016-04.pdf) in
the content `Reporting Categories` of Motions and Forces, `MF`, Energy,
`EN`, and Waves, `WA`. Each item is associated with a specific content
standard from the Massachusetts Curriculum Framework as well as an
underlying science `Practice Category` of Evidence Reasoning and
Modeling, `ERM`, Mathematics and Data, `MD`, or Investigations and
Questioning, `IQ`. The State Item Report provides the percentage of
points earned by students in a school for each item as well as the
percentage of points earned by all students in the state for each item.

The `HSPhy_NextGen_SchoolSum` data frame contains summary performance
results from 112 public schools across the commonwealth on the Next
Generation High School Introductory Physics MCAS, which was administered
in the Spring of 2022 and 2023. 87 schools tested students in both years
and 25 schools only tested students in 1 of the 2 testing years, with
27,745 students completing the exam.

For each school, there are values reported for 44 different variables
which consist of information from three broad categories

-   *School Characteristics*: This includes the name of the school and
    the size of the school, `School Size`, as determined by the number
    of students that completed the MCAS exam.

-   *Discipline-Specifc Performance Metrics*: This includes the
    percentage of points earned by students at a school for items each
    content `Reporting Category`, `MF%`, `EN%`, `WA%` and science
    `Practice Category` `ERM%`, `MD%`, `IQ%`, the difference between a
    school's percentage of points earned compared to the percentage of
    points earned by all students in the state (`MFDiff`, `ENDiff`,
    etc...), and the variability in a school's performance relative to
    the state by category as measured by the standard deviation of the
    school's `Diff` across categories (`SD MF Diff`, `SD EN Diff`,
    etc...).

-   *Aggregate Performance Level metrics*: This includes a school's
    percentage of students at each of the four `Performance Levels`,
    (`E%`: Exceeding Expectations, `M%`: Meeting Expectations, `PM%`:
    Partially Meeting Expectations, and `NM%`: Not Meeting
    Expectations), the difference between these percentages and the
    percentage of students in Massachusetts at each performance level
    (`EDiff`, `MDiff`, `PMDiff`, `NMDiff`), and an ordinal
    classification of school's, `EM Perf Stat` based on the percentage
    of students that were classified as Exceeding or Meeting
    expectations on the exam (`HighEM`, `HighM`, `Mid`, `Mid-Low`,
    `Low`).

See the `HSPhy_NextGenMCASDF` data frame summary and **codebook** for
further details about all variables.

# Hypothesis

```{=html}
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
```
::: blue
-   A school's percentage of students classified as `Exceeding`
    expectations on the Introductory Physics MCAS is negatively
    associated with a school's variance in performance relative to
    students in the state on `Mathematics and Data` items, `SD MD Diff`.

-   A school's summary performance on items in a given content
    `Reporting Category` as measured by `MF%`, `EN%`, and `WA%`, is
    positively associated with the `Reporting Category's` weight within
    the exam.
:::

# Descriptive Statistics

```{r}
#| label: dataframe setup
#| warning: false
#| message: false

#HSPhy_NextGen_SchoolSum
HSPhy_NextGen_SchoolSum<-HSPhy_NextGen_SchoolSum%>%
  ungroup()

#HSPhy_NextGen_SchoolSum
# HSPhy_NextGen_PerfDF
# HSPhy_NextGen_SchoolIT301DF

HSPhy_2023_SchoolSizeDF<-read_excel("data/2023_Physics_NextGenMCASItem.xlsx", skip = 1)%>%
  select(`School Name`, `School Code`, `Tested`)%>%
  mutate(`Tested` = as.integer(`Tested`))%>%
  select(`School Name`, `School Code`, `Tested`)

HSPhy_2022_SchoolSizeDF<-read_excel("data/2022_Physics_NextGenMCASItem.xlsx", skip = 1)%>%
  select(`School Name`, `School Code`, `Tested`)%>%
  mutate(`Tested` = as.integer(`Tested`))%>%
  select(`School Name`, `School Code`, `Tested`)


HSPhy_SchoolSize <- rbind(HSPhy_2023_SchoolSizeDF, HSPhy_2022_SchoolSizeDF)%>%
  mutate(count = 1)%>%
  group_by(`School Name`, `School Code`)%>%
  summarise(count = sum(count),
            `Tested` = sum(`Tested`))%>%
  mutate(`Tested Count` = round(`Tested`/count))%>%
  ungroup()
#HSPhy_SchoolSize
quantile <- quantile(HSPhy_SchoolSize$`Tested Count`)
HSPhy_Size<-HSPhy_SchoolSize%>%
  mutate(`School Size` = case_when(
    `Tested Count` <= quantile[2] ~ 0, # small
    `Tested Count` > quantile[2] &
      `Tested Count` <= quantile[3] ~ 1, #Low-Mid
    `Tested Count` > quantile[3] &
      `Tested Count` <= quantile[4] ~ 2, #Upper Mid
    `Tested Count` > quantile[4] &
      `Tested Count` <= quantile[5] ~ 3, # Large
  ))%>%
  # mutate(`School Size` = recode_factor(`School Size`,
  #                                           "Small" = "Small",
  #                                           "Low-Mid" = "Low-Mid",
  #                                           "Upper-Mid" = "Upper-Mid",
  #                                           "Large" = "Large",
  #                                           .ordered = TRUE))%>%
  select(`School Name`, `School Code`, `School Size`)


#HSPhy_Size

HSPhy_NextGen_SchoolSum<-HSPhy_NextGen_SchoolSum%>%
  left_join(HSPhy_Size, by = c("School Name" = "School Name", "School Code" = "School Code"))%>%
  mutate(`EMDiff` = `EDiff` + `MDiff`)%>%
  mutate(`EM Perf Stat` = case_when(
    `EDiff` > 0 & `EDiff` + `MDiff` > 0 ~ "HighEM",
    `EDiff` <= 0 & `EDiff` + `MDiff` > 0 ~ "HighM",
    #`EMDiff` > quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)[3] & 
      `EMDiff` <= 0  & `EMDiff` > -14 ~ "Mid",
    `EMDiff` <= -14 & `EMDiff` >= -33  ~ "Mid-Low",
    `EMDiff` < -33  ~ "Low"
   
  ))%>%
  mutate(`EM Perf Stat` = recode_factor(`EM Perf Stat`,
                                 "HighEM" = "HighEM",
                                 "HighM" = "HighM",
                                 "Mid" = "Mid",
                                 "Mid-Low" = "Mid-Low",
                                 "Low" = "Low",
                                 .ordered = TRUE))
HSPhy_NextGen_SchoolSum
#quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)

                                      

#summary(HSPhy_NextGen_SchoolSum)
print(summarytools::dfSummary(HSPhy_NextGen_SchoolSum,
                         varnumbers = FALSE,
                         plain.ascii  = FALSE,
                         style        = "grid",
                         graph.magnif = 0.70,
                        valid.col    = FALSE),
       method = 'render',
       table.classes = 'table-condensed')

```

## Key Variables

To explore the relationship between the distribution of school's
students' `Performance Level` and school's performance in content
categories, we examine the percentage of points earned by students at
schools as well as the standard deviation of the difference between
points earned by students at a school and points earned by students in
the state across `Reporting Categories` and `Practice Categories`. We
grouped schools by their `EM Perf Stat`, an ordinal variable classifying
schools by the percentage of students they have that were classified as
either Exceeding or Meeting expectations on the MCAS. These numbers seem
to suggest that items classified with the Science `Practice Category` of
`Mathematics and Data` seem to be more challenging to students than
those classified as `Evidence, Reasoning, and Modeling`. These practice
categories are strongly and equally emphasized within the exam; items
tagged with these categories account for **82%** of the available points
on the exam with exactly **41%** of available points coming from each
category.

When considering content `Reporting Categories`, there do not seem to be
discernible distinctions between `EM Perf Stat` and school's achievement
and performance across categories. All schools seem to perform the
strongest on `Motion and Forces` items, followed by `Energy`, and
weakest on `Waves` items. Notably, this is also the order of the
relative weights of the content areas within the exam; `MF`, `EN`, and
`WA` items account for **50%**, **30%**, and **20%** of exam points
respectively.

```{r}

 #quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)


 
HSPhy_NextGen_SchoolSum%>%
  group_by(`EM Perf Stat`)%>%
    summarise( `Mean MD%` = mean(`MD%`), 
              `Mean MD SD` = mean(`MD Diff SD`),
              `Mean ERM%` = mean(`ERM%`),
               `Mean ERM SD` = mean (`ERM Diff SD`))


HSPhy_NextGen_SchoolSum%>%
  group_by(`EM Perf Stat`)%>%
    summarise( `Mean MF%` = mean(`MF%`), 
              `Mean MF SD` = mean(`MF Diff SD`),
              `Mean EN%` = mean(`EN%`),
               `Mean EN SD` = mean (`EN Diff SD`),
              `Mean WA%` = mean(`WA%`),
               `Mean WA SD` = mean (`WA Diff SD`)
              )



```

# Visualization

## Distribution of Performance Level %

When examining the statewide performance distribution, we can see from
the right-skew that it is rare for schools to have high percentages of
students classified as `Not Meeting` expectations and even rarer for
schools to have high percentages of students classified as `Exceeding`
expectations.

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`E%`, `M%`, `PM%`, `NM%`)%>%
  pivot_longer(c(1:4), names_to = "Performance Level", values_to = "% Students")%>%
   ggplot( aes(x=`% Students`, color=`Performance Level`, fill=`Performance Level`)) +
    geom_histogram(alpha=0.6, binwidth = 15) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Performance Level`)+
      labs( y = "",
            title = "School Performance Level Distribution",
            x = "% Students at Performance Level",
            caption = "NextGen HS Physics MCAS")


```

## Distribution of School Performance and Variability by Practice Cat

Although `Mathematics and Data` and `Evidence, Reasoning, and Modeling`
items have strong and equal weighting in the HS Introductory Physics
exam, student performance distributions are noticeably different across
these practice categories.

```{r}

HSPhy_NextGen_SchoolSum%>%
  select(`ERM%`, `MD%`)%>%
  pivot_longer(c(1:2), names_to = "Practice Cat", values_to = "% Points")%>%
   ggplot( aes(x=`% Points`, color=`Practice Cat`, fill=`Practice Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Practice Cat`)+
      labs( y = "",
            title = "School Performance by Practice Category",
            x = "% Points Earned",
            caption = "NextGen HS Physics MCAS")
  #ggtitle("Practice Category Performance")


```

When considering the variability of a school's performance on items
relative to the state by `Practice Category`, `SD MD Diff`, and
`SD ERM Diff`, we can see that `Mathematics and Data` is skewed more to
the right.

```{r}
  
  HSPhy_NextGen_SchoolSum%>%
  select(`ERM Diff SD`, `MD Diff SD`)%>%
  pivot_longer(c(1:2), names_to = "Practice Cat", values_to = "SD Diff")%>%
   ggplot( aes(x=`SD Diff`, color=`Practice Cat`, fill=`Practice Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
   # theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
      labs( y = "",
            title = "School Performance Variation by Practice Category",
            x = "SD Diff",
            caption = "NextGen HS Physics MCAS") +
    facet_wrap(~`Practice Cat`)
```

## Mathematics and Data vs. Evidence Reasoning and Modeling (Practice Category)

These images, seem to suggest that schools with the **highest**
percentage of students classified as `Exceeding` expectations on the
MCAS have the **lowest** levels of variation in performance on
`Mathematics and Data` Items and schools with the **lowest** percentage
of students classified as `Exceeding` expectations on the MCAS have the
**highest** levels of variation in performance on
`Mathematics and Data Items`.

```{r}


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM Diff SD`, `MD Diff SD` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`SD Diff`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    
    theme(
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      #axis.text.x=element_blank()
    ) +
 
    labs( y = "SD Diff",
            title = "Student Performance Variation by Practice Category",
            x = "",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Practice Cat`)


      


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM Diff SD`, `MD Diff SD` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `Practice Cat`, y=`SD Diff`, fill= `Practice Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
     
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "SD Diff",
            title = "Student Practice Cat. Variation by Achievement Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`)
```

These images, seem to suggest that students at all schools seem to have
more difficulty with `Mathematics and Data` items as compared to
`Evidence, Reasoning, and Modeling Items`.

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM%`, `MD%` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`%Points`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
      
      plot.title = element_text(size=11)
    ) +
    labs( y = "%Points Earned",
            title = "Student Practice Cat. Achievement by Performance Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`Practice Cat`)
```

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM%`, `MD%` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
  ggplot( aes(x= `Practice Cat`, y=`%Points`, fill= `Practice Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
      
      plot.title = element_text(size=11)
    ) +
    labs( y = "%Points Earned",
            title = "Student Practice Cat. Achievement by Performance Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`, scale ="free_y")

```

## Distribution of School Performance and Variability by Reporting Cat

Here we can visualize the variability of a school's performance on items
partitioned by Content `Reporting Category` of `Motion and Forces`,
`Energy`, and `Waves` via: `MF%`/`SD MF Diff`, `EN%`/`SD EN Diff`, and
`WA%`/`SD WA Diff`.

```{r}
  HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x=`SD Diff`, color=`Report Cat`, fill=`Report Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
      labs( y = "",
            title = "School Performance Variation by Content Reporting Category",
            x = "SD Diff",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Report Cat`)

```

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF%`, `EN%`, `WA%` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "% Points")%>%
 ggplot( aes(x=`% Points`, color=`Report Cat`, fill=`Report Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Report Cat`)+
      labs( y = "",
            title = "Student Performance by Content Reporting Category",
            x = "% Points Earned",
            caption = "NextGen HS Physics MCAS")
  #ggtitle("Practice Category Performance")


```

## Motion and Forces vs. Energy vs. Waves (Reporting Category)

These images suggest that most schools exhibit similar levels of
variability in performance relative to the state across all reporting
categories. Schools with the ***lowest percentage*** of students
`Exceeding` expectations exhibit ***high variability*** in performance
across all content reporting categories, but seem to have lower
variability on `Waves` items.

```{r}

HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`SD Diff`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    
    theme(
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
 
    labs( y = "SD Diff",
            title = "School Performance Variation by Content Reporting Category",
            x = "",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Report Cat`)


      


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `Report Cat`, y=`SD Diff`, fill= `Report Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
     
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "SD Diff",
            title = "School Content Reporting Cat. Variation by Achievement Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`)


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF%`, `EN%`, `WA%` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "% Points")%>%
  ggplot( aes(x= `Report Cat`, y=`% Points`, fill= `Report Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(

      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "Report Cat%",
            title = "School Content Reporting Cat. Performance by Achievement Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`)


```
```{r}
HSPhy_NextGen_SchoolSum<-HSPhy_NextGen_SchoolSum%>%
  ungroup()

HSPhy_NextGen_SchoolSum<-HSPhy_NextGen_SchoolSum%>%
  mutate(`EorM%` = `E%` + `M%`)

#HSPhy_NextGen_SchoolSum

```

## Visualization of School Size, EorM%, MD Diff, ERM Diff

To explore the relationship between the variance in a schools' `Diff`
compared to the state on Mathematics and Data items, `MD Diff SD` and a
school's percentage of students meeting or exceeding expectations on the
MCAS, `EorM%`, we also will considered the impact
of `School Size`, `0`: Smallest, `3`: Largest schools, as a control. It appears from our visuals that
`Small` schools have a higher variation in mathematics and data items
smaller school's typically perform worse on Mathematics and Data and overall on the
MCAS compared to larger schools.

```{r}


HSPhy_NextGen_SchoolSum%>%
  group_by(`School Size`)%>%
  summarize(
    `Mean EorM%` = mean(`EorM%`),
      `Mean MD%` = mean(`MD%`),
     `Mean MD Diff SD` = mean(`MD Diff SD`),
     `Mean ERM%` = mean(`ERM%`),
     `Mean ERM Diff SD` = mean(`ERM Diff SD`)
             )




```

```{r}


HSPhy_NextGen_SchoolSum%>%
  select(`School Size`, `MD Diff SD`, `ERM Diff SD` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `Practice Cat`, y=`SD Diff`, fill= `Practice Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(

      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "SD Diff",
            title = "Student Practice Cat. Variation by School Size",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`School Size`, scale = "free")


HSPhy_NextGen_SchoolSum%>%
  select(`School Size`, `MD%`, `ERM%` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
  ggplot( aes(x= `Practice Cat`, y=`%Points`, fill= `Practice Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(

      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "% Points Earned",
            title = "Student Practice Cat. Achievement by School Size",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`School Size`)

```

However, when you group the schools by `EM Perf Stat`, you find that the
highest performing, `High EM`, `Small` schools have a higher percentage
of students meeting or exceeding expectations.

Across all sizes, it seems that the weakest performing schools have more
variation in mathematics and data and the strongest performing schools
have less variability in `Mathematics and Data` than in
`Evidence, Reasoning, and Modeling`.

```{r}

# Faceted by performance level
  HSPhy_NextGen_SchoolSum%>%
  group_by(`School Size`, `EM Perf Stat`)%>%
  summarize(`Mean EorM%` = mean(`EorM%`),
      `Mean MD%` = mean(`MD%`),
     `Mean MD Diff SD` = mean(`MD Diff SD`),
     `Mean ERM%` = mean(`ERM%`),
     `Mean ERM Diff SD` = mean(`ERM Diff SD`)
             )

# HSPhy_NextGen_SchoolSum%>%
#   select(`School Size`, `EorM%`, `MD Diff SD`, `EM Perf Stat` )%>%
#   #pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
#   ggplot( aes(x= `School Size`, y=`EorM%`, fill= `School Size`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     #theme_ipsum() +
#     theme(
#      
#       plot.title = element_text(size=11),
#       axis.title.x=element_blank(),
#       axis.text.x=element_blank()
#     ) +
#      labs( y = "SD Diff",
#             title = "Students Meeting or Exceeding Expectations . Variation by Achievement Level",
#             x = "",
#             caption = "NextGen HS Physics MCAS") +
#     #xlab("")+
#   facet_wrap(~`EM Perf Stat`, scales = "free")


```
# Hypothesis Testing

## Hypothesis 1: Variation in Mathematics

```{=html}
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
```
::: blue
-   A school's percentage of students classified as
    `Exceeding or Meeting` expectations on the Introductory Physics MCAS
    is negatively associated with a school's variance in performance
    relative to students in the state on `Mathematics and Data` items,
    `SD MD Diff`.
:::

## Regression Model

### MD Diff Alone and with ERM

```{r}
fit_md = lm(`EorM%` ~ (`MD Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_md)

fit_size = lm(`EorM%` ~ (`School Size`), data = HSPhy_NextGen_SchoolSum)
summary(fit_size)

fit_md_erm = lm(`EorM%` ~ (`MD Diff SD` + `ERM Diff SD` + `MD Diff SD` * `ERM Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_md_erm)

fit_md_size1 = lm(`EorM%` ~ (`MD Diff SD` + `Tested Students`), data = HSPhy_NextGen_SchoolSum)
summary(fit_md_size1)

fit_md_size2 = lm(`EorM%` ~ (`MD Diff SD` + `School Size`), data = HSPhy_NextGen_SchoolSum)
summary(fit_md_size2)

```

### MD Diff and School Size with Interaction

This states that MD is significant and `Tested Students` is
statistically significant?

```{r}

fit_md_size_interact = lm(`EorM%` ~ (`MD Diff SD` + `School Size` + `MD Diff SD` * `School Size`), data = HSPhy_NextGen_SchoolSum)
summary(fit_md_size_interact)

```
### Robustness Checks
```{r}
AIC(fit_md_size_interact)
BIC(fit_md_size_interact)
#PRESS(fit_md_size_interact)

```
### Visualizations

***Question, do I plot the actual model with coefficients?*** \*\*\* How
do I include School Size in the visual?\*\*\*

#### EM Perf Stat Color: MD Diff SD, EorM%

```{r}
ggplot(data = HSPhy_NextGen_SchoolSum, aes(x = `MD Diff SD`, y = `EorM%`, color = `School Size`)) +
  geom_point() +
  geom_smooth(method="lm", se=T) 
```

```{r}
ggplot(data = HSPhy_NextGen_SchoolSum, aes(x = `MD Diff SD`, y = `EorM%`, color = `EM Perf Stat`)) +
  geom_point() +
  geom_smooth(method="lm", se=T) 
```


## Hypothesis 2: Reporting Cateogy and School Performance

```{=html}
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
```
::: blue
-   A school's summary performance on items in a given content
    `Reporting Category` as measured by `MF%`, `EN%`, and `WA%`, is
    positively associated with the `Reporting Category's` weight within
    the exam.
:::

## SD-Diff Reporting Categories

```{r}
ANOVA_WA <- aov(`WA Diff SD` ~ `EM Perf Stat`, data=HSPhy_NextGen_SchoolSum)

summary(ANOVA_WA)


ANOVA_EN <- aov(`EN Diff SD` ~ `EM Perf Stat`, data=HSPhy_NextGen_SchoolSum)

summary(ANOVA_EN)

ANOVA_MF <- aov(`MF Diff SD` ~ `EM Perf Stat`, data=HSPhy_NextGen_SchoolSum)

summary(ANOVA_MF)

 HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
   group_by(`Report Cat`, `EM Perf Stat`)%>%
   summarize(`SD SD Diff` = sd(`SD Diff`, na.rm = TRUE))
 
 

  HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM Diff SD`, `MD Diff SD` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
   group_by(`Practice Cat`, `EM Perf Stat`)%>%
   summarize(`SD SD Diff` = sd(`SD Diff`, na.rm = TRUE),
             `Mean SD Diff` = mean(`SD Diff`, na.rm = TRUE))
  
  
```

## Reporting Category DIFF alone and with interactions

### EN

```{r}
fit_en = lm(`EorM%` ~ (`EN Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_en)

```

### MF

```{r}
fit_mf = lm(`EorM%` ~ (`MF Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_mf)

```

### WA

```{r}
fit_wa = lm(`EorM%` ~ (`WA Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_wa)

```

### MF/EN

```{r}

fit_mf_en = lm(`EorM%` ~ (`MF Diff SD` + `EN Diff SD` + `MF Diff SD`*`EN Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_mf_en)

```

### MF/WA interact

```{r}

fit_mf_wa = lm(`EorM%` ~ (`MF Diff SD`) + `WA Diff SD` + `MF Diff SD`*`WA Diff SD`, data = HSPhy_NextGen_SchoolSum)
summary(fit_mf_wa)

```

## Practice Cat Interacting with Reporting Cat

### MD_WA_interact

```{r}


fit_md_wa_interact = lm(`EorM%` ~ (`MD Diff SD` + `WA Diff SD`  + `MD Diff SD`*`WA Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_md_wa_interact)
```

### Model Comparison: MD vs. MD_WA_interact: Anova F test

```{r}
anova(fit_md, fit_md_wa_interact)
```

### MD/: MF, not significant...possible multicollinearity

```{r}


fit_md_mf_interact = lm(`EorM%` ~ (`MD Diff SD` + `MF Diff SD` + `MD Diff SD`*`MF Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_md_mf_interact)


```

### MD/EN, EN not significant

```{r}

fit_md_en_interact = lm(`EorM%` ~ (`MD Diff SD` + `EN Diff SD` + `MD Diff SD`*`EN Diff SD`), data = HSPhy_NextGen_SchoolSum)
summary(fit_md_en_interact)


```

### MD-Diff, WA Diff, and School Size

```{r}
fit_md_wa_size_interact = lm(`EorM%` ~ `MD Diff SD` + `WA Diff SD` + `School Size` + `MD Diff SD`*`School Size`, data = HSPhy_NextGen_SchoolSum)
summary(fit_md_wa_size_interact)


```

### Model Comparison: MD_WA_interact vs. MD_WA_Size_interact: Anova F test

```{r}
anova(fit_md_wa_interact, fit_md_wa_size_interact)

```

### Model Comparison: MD_WA_interact vs. MD_WA_Size_interact: BIC

```{r}
summary(fit_md_wa_interact)
summary(fit_md_wa_size_interact)
summary(fit_md_size_interact)
BIC(fit_md_wa_interact)
BIC(fit_md_wa_size_interact)
BIC(fit_md_size_interact)
AIC(fit_md_wa_interact)
AIC(fit_md_wa_size_interact)
AIC(fit_md_size_interact)
```

# Diagnostic of Final Model: MD_WA_Size_interact

Cook's distance says the 16th, 47th, and 104th observations are in
violation, since $\frac{4}{n} \approx .04$. When we examine our data
frame, we see the 16th. represents a `Low` Performing School that is
`Large` in size with low `MD%` and high `MD Diff SD`.

```{r}
plot(fit_md_wa_size_interact, which = 1:6)
cooks = 4/112
cooks

test4<-HSPhy_NextGen_SchoolSum%>%
  filter(`School Code` != "00350537")

test5<-test4%>%
  filter(`School Code` != "00350537")

test5<-test5%>%
  filter(`School Code` != "01520505")

test5<-test5%>%
  filter(`School Code` != "08010605")

test5



```

### Adjusted model with outliers removed

```{r}
adj_fit_md_wa_size_interact <- lm(formula = `EorM%` ~ `MD Diff SD` + `WA Diff SD` + `School Size` + 
    `MD Diff SD` * `School Size`, data = test5)

summary(adj_fit_md_wa_size_interact)
plot(adj_fit_md_wa_size_interact, which = 1:6)
BIC(adj_fit_md_wa_size_interact)
AIC(adj_fit_md_wa_size_interact)
test5
4/109
```

We need to remove the 29th, 32nd and 40th observation.

```{r}

test6<-test5%>%
  filter(`School Code` != "00610021")%>%
  filter(`School Code` != "00860505")%>%
  filter(`School Code` != "01250505")
  
  
adj2_fit_md_wa_size_interact <- lm(formula = `EorM%` ~ `MD Diff SD` + `WA Diff SD` + `School Size` + 
    `MD Diff SD` * `School Size`, data = test6)

summary(adj2_fit_md_wa_size_interact)
plot(adj2_fit_md_wa_size_interact, which = 1:6)
BIC(adj2_fit_md_wa_size_interact)
AIC(adj2_fit_md_wa_size_interact)

```

# Visuals

## MD + WA + School Size

\*\*\* How do I include School Size in the visual?\*\*\*

```{r}


ggplot(data = test6, aes(x = `MD Diff SD`, y = `EorM%`, color = `EM Perf Stat`)) +
  geom_point() +
  geom_smooth(method="lm", se=T)



```

```{r}


ggplot(data = HSPhy_NextGen_SchoolSum, aes(x = `MD Diff SD` +  `WA Diff SD`, y = `EorM%`, color = `School Size`)) +
  geom_point() +
  geom_smooth(method="lm", se=T)



```

# References
