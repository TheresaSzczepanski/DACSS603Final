---
title: "DACSS603Final"
author: "Theresa Szczepanski"
desription: "MCAS G9 Science Analysis"
date: "10/22/2023"

format:
  html:
    embed-resources: true
    self-contained-math: true
    df-print: paged
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
    
bibliography: references.bib

editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: setup
#| warning: false
#| message: false

source('dependencies.R')
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

# Research Questions

The Massachusetts Education Reform Act in 1993 was passed in the context
of a national movement toward education reform throughout the United
States. As early as 1989 there were calls to establish national
curriculum standards as a way to improve student college and career
readiness skills and close poverty gaps [@Greer18]. Massachusetts
Comprehensive Assessment System (MCAS) tests were introduced as part of
the Massachusetts Education Reform Act.

The MCAS tests are a significant tool for educational equity. Scores on
the Grade 10 Math MCAS test "predict longer-term educational attainments
and labor market success, above and beyond typical markers of student
advantage" and differences among students are largely and "sometimes completely
accounted for" by differences in 10th grade MCAS scores and educational attainments.
[@Boats20].

With the introduction of the new Common Core standards and accountability testing came the demand
for aligned curricular materials and teaching practices. Research
indicates that the choice of instructional materials can have an impact
"as large as or larger than the impact of teacher quality" [@Blindly12].
Massachusetts, along with Arkansas, Delaware, Kentucky, Louisiana,
Maryland, Mississippi, Nebraska, New Mexico, Ohio, Rhode Island,
Tennessee, and Texas belongs to the Council of Chief State School
Officers' (CCSO), [High Quality Instructional Materials and Professional
Development
network](https://learning.ccsso.org/high-quality-instructional-materials)
which aims to close the "opportunity gap" among students by ensuring that 
every teacher has access to high-quality, standards aligned
instructional materials and receives relevant professional development
to support their use of these materials [@IMPD21].

All Massachusetts Public School students must complete a High School science MCAS
exam providing a wealth of standardized data on students' discipline specific skill development.
All schools receive annual summary reports on student performance. Significant work has been 
done using the MCAS achievement data and the Student Opportunity Act to identify achievement gaps and address funding inequities across the Commonwealth [@Boats20]. With funding gaps outlined in the late 1990's 
closing, one could consider how the MCAS data could be leveraged to support the state's 
current high quality instructional materials initiatives. The state compiles school's performance
disaggregated by each MCAS question item [@MCASIT].

Using the curricular information provided in state wide Next Generation MCAS High School Introductory Physics
Item reports together with school-level student performance data, we hope to address the following broad
questions:

```{=html}
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
```
::: blue
-   Is there a relationship between differences in a school's performance
    across Science Practice Categories and a school's overall achievement on the Introductory Physics exam?

-   How can trends in a school's performance be used to provide schools with guidance on
    discipline-specific curricular areas to target to improve student achievement?
:::


In this report, I will analyze the High School Introductory Physics Next
Generation [Massachusetts Comprehensive Assessment System
(MCAS)](https://www.doe.mass.edu/mcas/default.html) tests results for
Massachusetts public schools.

Data for the study were drawn from DESEâ€™s Next Generation MCAS Test
[Achievement Results statewide report](https://profiles.doe.mass.edu/statereport/mcas.aspx),
[Item Analysis statewide report](https://profiles.doe.mass.edu/statereport/nextgenmcas_item.aspx), and 
the [MCAS digital item library](https://mcas.digitalitemlibrary.com/home?subject=Science&grades=Physics&view=ALL). The Next Generation High School Introductory Physics MCAS assessment consists of 42 multiple choice and constructed response items that assess students on Physical Science standards from the [2016 STE Massachusetts
Curriculum Framework](https://www.doe.mass.edu/frameworks/scitech/2016-04.pdf) 
in the content `Reporting Categories` of Motions and Forces, `MF`, 
Energy, `EN`, and Waves, `WA`. Each item is associated with a specific content standard from the Massachusetts Curriculum Framework as well as an underlying science `Practice Category` of 
Evidence Reasoning and Modeling, `ERM`, Mathematics and Data, `MD`, or Investigations 
and Questioning, `IQ`. The State Item Report provides the percentage of points earned by 
students in a school for each item as well as the percentage of points earned by 
all students in the state for each item.


The `HSPhy_NextGen_SchoolSum` data frame contains summary performance results from
112 public schools across the commonwealth on the Next Generation High
School Introductory Physics MCAS, which was administered in the Spring
of 2022 and 2023. 87 schools
tested students in both years and 25  schools only tested
students in 1 of the 2 testing years, with 27,745 students completing the exam.

For each school, there are values reported for 44 different variables
which consist of information from three broad categories

-   *School Characteristics*: This includes the name of the school and
    the size of the school, `School Size`, as determined by the number of students that
    completed the MCAS exam.

-   *Discipline-Specifc Performance Metrics*: This
    includes the percentage of points earned by students at a school for items 
    each content `Reporting Category`, `MF%`, `EN%`, `WA%` and science `Practice Category` 
    `ERM%`, `MD%`, `IQ%`, the difference between a school's percentage of points 
    earned compared to the percentage of points earned by all students in the state (`MFDiff`, `ENDiff`, etc...),     and the variability in a school's performance relative to the state by category as 
    measured by the standard deviation of the school's `Diff` across categories (`SD MF Diff`, `SD EN Diff`, etc...).
    

-   *Aggregate Performance Level metrics*: This includes a school's percentage
    of students at each of the four `Performance Levels`, (`E%`: Exceeding
    Expectations, `M%`: Meeting Expectations, `PM%`: Partially Meeting
    Expectations, and `NM%`: Not Meeting Expectations),
    the difference between these percentages and the percentage of students 
    in Massachusetts at each performance level (`EDiff`, `MDiff`, `PMDiff`, `NMDiff`),
    and an ordinal classification of school's, `EM Perf Stat` based on the percentage of students that were 
    classified as Exceeding or Meeting expectations on the exam (`HighEM`, `HighM`, `Mid`, `Mid-Low`, `Low`).



See the `HSPhy_NextGenMCASDF` data frame summary and **codebook** for
further details about all variables.

# Hypothesis

```{=html}
<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
```
::: blue
-   A school's percentage of students classified as `Exceeding` expectations on the
    Introductory Physics MCAS is negatively associated with a school's variance in
    performance relative to students in the state on `Mathematics and Data` items, `SD MD Diff`.

-   A school's summary performance on items in a given content `Reporting Category` as measured by `MF%`, `EN%`, and `WA%`, is positively associated with the `Reporting Category's` weight within the exam.
:::



# Descriptive Statistics

```{r}
#| label: dataframe setup
#| warning: false
#| message: false

#HSPhy_NextGen_SchoolSum
HSPhy_NextGen_SchoolSum<-HSPhy_NextGen_SchoolSum%>%
  ungroup()

#HSPhy_NextGen_SchoolSum
# HSPhy_NextGen_PerfDF
# HSPhy_NextGen_SchoolIT301DF

HSPhy_2023_SchoolSizeDF<-read_excel("data/2023_Physics_NextGenMCASItem.xlsx", skip = 1)%>%
  select(`School Name`, `School Code`, `Tested`)%>%
  mutate(`Tested` = as.integer(`Tested`))%>%
  select(`School Name`, `School Code`, `Tested`)

HSPhy_2022_SchoolSizeDF<-read_excel("data/2022_Physics_NextGenMCASItem.xlsx", skip = 1)%>%
  select(`School Name`, `School Code`, `Tested`)%>%
  mutate(`Tested` = as.integer(`Tested`))%>%
  select(`School Name`, `School Code`, `Tested`)


HSPhy_SchoolSize <- rbind(HSPhy_2023_SchoolSizeDF, HSPhy_2022_SchoolSizeDF)%>%
  mutate(count = 1)%>%
  group_by(`School Name`, `School Code`)%>%
  summarise(count = sum(count),
            `Tested` = sum(`Tested`))%>%
  mutate(`Tested Count` = round(`Tested`/count))%>%
  ungroup()
#HSPhy_SchoolSize
quantile <- quantile(HSPhy_SchoolSize$`Tested Count`)
HSPhy_Size<-HSPhy_SchoolSize%>%
  mutate(`School Size` = case_when(
    `Tested Count` <= quantile[2] ~ "Small",
    `Tested Count` > quantile[2] &
      `Tested Count` <= quantile[3] ~ "Low-Mid",
    `Tested Count` > quantile[3] &
      `Tested Count` <= quantile[4] ~ "Upper-Mid",
    `Tested Count` > quantile[4] &
      `Tested Count` <= quantile[5] ~ "Large",
  ))%>%
  mutate(`School Size` = recode_factor(`School Size`,
                                            "Small" = "Small",
                                            "Low-Mid" = "Low-Mid",
                                            "Upper-Mid" = "Upper-Mid",
                                            "Large" = "Large",
                                            .ordered = TRUE))%>%
  select(`School Name`, `School Code`, `School Size`)


#HSPhy_Size

HSPhy_NextGen_SchoolSum<-HSPhy_NextGen_SchoolSum%>%
  left_join(HSPhy_Size, by = c("School Name" = "School Name", "School Code" = "School Code"))%>%
  mutate(`EMDiff` = `EDiff` + `MDiff`)%>%
  mutate(`EM Perf Stat` = case_when(
    `EDiff` > 0 & `EDiff` + `MDiff` > 0 ~ "HighEM",
    `EDiff` <= 0 & `EDiff` + `MDiff` > 0 ~ "HighM",
    #`EMDiff` > quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)[3] & 
      `EMDiff` <= 0  & `EMDiff` > -14 ~ "Mid",
    `EMDiff` <= -14 & `EMDiff` >= -33  ~ "Mid-Low",
    `EMDiff` < -33  ~ "Low"
   
  ))%>%
  mutate(`EM Perf Stat` = recode_factor(`EM Perf Stat`,
                                 "HighEM" = "HighEM",
                                 "HighM" = "HighM",
                                 "Mid" = "Mid",
                                 "Mid-Low" = "Mid-Low",
                                 "Low" = "Low",
                                 .ordered = TRUE))
HSPhy_NextGen_SchoolSum
#quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)

                                      

#summary(HSPhy_NextGen_SchoolSum)
print(summarytools::dfSummary(HSPhy_NextGen_SchoolSum,
                         varnumbers = FALSE,
                         plain.ascii  = FALSE,
                         style        = "grid",
                         graph.magnif = 0.70,
                        valid.col    = FALSE),
       method = 'render',
       table.classes = 'table-condensed')

```

## Key Variables

To explore the relationship between the distribution of school's students' `Performance Level`
and school's performance in content categories, we examine the percentage of points 
earned by students at schools as well as the standard deviation of the difference between points earned by students at a school and points earned by students in the state across `Reporting Categories` and `Practice Categories`. We grouped schools by their `EM Perf Stat`, an ordinal variable classifying schools 
by the percentage of students they have that were classified as either Exceeding or 
Meeting expectations on the MCAS. These numbers seem to suggest that items classified 
with the Science `Practice Category` of `Mathematics and Data` seem to be more challenging to students than those 
classified as `Evidence, Reasoning, and Modeling`. These practice categories are strongly and equally emphasized within the exam; items tagged with these categories account for **82%** of the available points on the exam with exactly **41%** of available points coming from each category.

When considering content `Reporting Categories`, there do not seem to be discernible distinctions between `EM Perf Stat` and school's achievement and performance 
across categories. All schools seem to perform the strongest on `Motion and Forces` items, followed by `Energy`, and weakest on `Waves` items. Notably, this is also the order of the relative weights of the content areas within the exam; `MF`, `EN`, and `WA` items account for **50%**, **30%**, and **20%** of exam points respectively.

```{r}

 #quantile(HSPhy_NextGen_SchoolSum$`EMDiff`)


 
HSPhy_NextGen_SchoolSum%>%
  group_by(`EM Perf Stat`)%>%
    summarise( `Mean MD%` = mean(`MD%`), 
              `Mean MD SD` = mean(`MD Diff SD`),
              `Mean ERM%` = mean(`ERM%`),
               `Mean ERM SD` = mean (`ERM Diff SD`))


HSPhy_NextGen_SchoolSum%>%
  group_by(`EM Perf Stat`)%>%
    summarise( `Mean MF%` = mean(`MF%`), 
              `Mean MF SD` = mean(`MF Diff SD`),
              `Mean EN%` = mean(`EN%`),
               `Mean EN SD` = mean (`EN Diff SD`),
              `Mean WA%` = mean(`WA%`),
               `Mean WA SD` = mean (`WA Diff SD`)
              )



```

# Visualization

## Distribution of Performance Level %

When examining the statewide performance distribution, we can see from the right-skew 
that it is rare for schools to have high percentages of students classified as `Not Meeting` expectations
and even rarer for schools to have high percentages of students classified as `Exceeding` expectations.

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`E%`, `M%`, `PM%`, `NM%`)%>%
  pivot_longer(c(1:4), names_to = "Performance Level", values_to = "% Students")%>%
   ggplot( aes(x=`% Students`, color=`Performance Level`, fill=`Performance Level`)) +
    geom_histogram(alpha=0.6, binwidth = 15) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Performance Level`)+
      labs( y = "",
            title = "School Performance Level Distribution",
            x = "% Students at Performance Level",
            caption = "NextGen HS Physics MCAS")


```

## Distribution of School Performance and Variability by Practice Cat

Although `Mathematics and Data` and `Evidence, Reasoning, and Modeling` items 
have strong and equal weighting in the HS Introductory Physics exam, student performance distributions
are noticeably different across these practice categories.

```{r}

HSPhy_NextGen_SchoolSum%>%
  select(`ERM%`, `MD%`)%>%
  pivot_longer(c(1:2), names_to = "Practice Cat", values_to = "% Points")%>%
   ggplot( aes(x=`% Points`, color=`Practice Cat`, fill=`Practice Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Practice Cat`)+
      labs( y = "",
            title = "School Performance by Practice Category",
            x = "% Points Earned",
            caption = "NextGen HS Physics MCAS")
  #ggtitle("Practice Category Performance")


```


When considering the variability of a school's performance on items relative 
to the state by `Practice Category`, `SD MD Diff`, and `SD ERM Diff`, we can 
see that `Mathematics and Data` is skewed more to the right.

```{r}
  
  HSPhy_NextGen_SchoolSum%>%
  select(`ERM Diff SD`, `MD Diff SD`)%>%
  pivot_longer(c(1:2), names_to = "Practice Cat", values_to = "SD Diff")%>%
   ggplot( aes(x=`SD Diff`, color=`Practice Cat`, fill=`Practice Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
   # theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
      labs( y = "",
            title = "School Performance Variation by Practice Category",
            x = "SD Diff",
            caption = "NextGen HS Physics MCAS") +
    facet_wrap(~`Practice Cat`)
```
## Mathematics and Data vs. Evidence Reasoning and Modeling (Practice Category)

These images, seem to suggest that schools with the **highest** percentage of students classified as `Exceeding` 
expectations on the MCAS have the **lowest** levels of variation in performance on 
`Mathematics and Data` Items and schools with the **lowest** percentage of students classified as 
`Exceeding` expectations on the MCAS have the **highest** levels of variation in performance
on `Mathematics and Data Items`.

```{r}


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM Diff SD`, `MD Diff SD` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`SD Diff`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    
    theme(
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      #axis.text.x=element_blank()
    ) +
 
    labs( y = "SD Diff",
            title = "Student Performance Variation by Practice Category",
            x = "",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Practice Cat`)


      


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM Diff SD`, `MD Diff SD` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `Practice Cat`, y=`SD Diff`, fill= `Practice Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
     
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "SD Diff",
            title = "Student Practice Cat. Variation by Achievement Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`)
```
These images, seem to suggest that students at all schools seem to have 
more difficulty with `Mathematics and Data` items as compared to `Evidence, Reasoning, 
and Modeling Items`.

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM%`, `MD%` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`%Points`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
      
      plot.title = element_text(size=11)
    ) +
    labs( y = "%Points Earned",
            title = "Student Practice Cat. Achievement by Performance Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`Practice Cat`)
```

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `ERM%`, `MD%` )%>%
  pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
  ggplot( aes(x= `Practice Cat`, y=`%Points`, fill= `Practice Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
      
      plot.title = element_text(size=11)
    ) +
    labs( y = "%Points Earned",
            title = "Student Practice Cat. Achievement by Performance Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`, scale ="free_y")
# HSPhy_NextGen_SchoolSum%>%
#   select(`EM Perf Stat`, `ERMDiff`, `MDDiff` )%>%
#   pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
#   ggplot( aes(x= `EM Perf Stat`, y=`%Points`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     #theme_ipsum() +
#     theme(
#       
#       plot.title = element_text(size=11)
#     ) +
#     labs( y = "%Points Earned",
#             title = "Student Practice Cat. Achievement by Performance Level",
#             x = "",
#             caption = "NextGen HS Physics MCAS") +
#     #xlab("")+
#   facet_wrap(~`Practice Cat`)

# HSPhy_NextGen_SchoolSum%>%
#   select(`EM Perf Stat`, `ERM%`, `MD%` )%>%
#   pivot_longer(c(2:3), names_to = "Practice Cat", values_to = "%Points")%>%
#   ggplot( aes(x= `Practice Cat`, y=`%Points`, fill= `Practice Cat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     #theme_ipsum() +
#     theme(
#       
#       plot.title = element_text(size=11)
#     ) +
#     labs( y = "%Points Earned",
#             title = "Student Practice Cat. Achievement by Performance Level",
#             x = "",
#             caption = "NextGen HS Physics MCAS") +
#     #xlab("")+
#   facet_wrap(~`EM Perf Stat`, scale ="free_y")
  

# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`MD%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")

# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`MD Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`ERM Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`ERM%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")


```



## Distribution of School Performance and Variability by Reporting Cat

Here we can visualize the variability of a school's performance on items partitioned 
by Content `Reporting Category` of `Motion and Forces`, `Energy`, and `Waves` via:
`MF%`/`SD MF Diff`,  `EN%`/`SD EN Diff`, and `WA%`/`SD WA Diff`.


```{r}
  HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x=`SD Diff`, color=`Report Cat`, fill=`Report Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
      labs( y = "",
            title = "School Performance Variation by Content Reporting Category",
            x = "SD Diff",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Report Cat`)

```

```{r}
HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF%`, `EN%`, `WA%` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "% Points")%>%
 ggplot( aes(x=`% Points`, color=`Report Cat`, fill=`Report Cat`)) +
    geom_histogram(alpha=0.6, binwidth = 3) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    #theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
  
    facet_wrap(~`Report Cat`)+
      labs( y = "",
            title = "Student Performance by Content Reporting Category",
            x = "% Points Earned",
            caption = "NextGen HS Physics MCAS")
  #ggtitle("Practice Category Performance")


```



## Motion and Forces vs. Energy vs. Waves (Reporting Category)

These images suggest that most schools exhibit similar levels of variability 
in performance relative to the state across all reporting categories. Schools with the ***lowest percentage*** of students `Exceeding` expectations exhibit ***high variability*** in performance across all content reporting 
categories, but seem to have lower variability on `Waves` items.


```{r}

HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `EM Perf Stat`, y=`SD Diff`, fill= `EM Perf Stat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    
    theme(
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
 
    labs( y = "SD Diff",
            title = "School Performance Variation by Content Reporting Category",
            x = "",
            caption = "NextGen HS Physics MCAS") +
  facet_wrap(~`Report Cat`)


      


HSPhy_NextGen_SchoolSum%>%
  select(`EM Perf Stat`, `MF Diff SD`, `EN Diff SD`, `WA Diff SD` )%>%
  pivot_longer(c(2:4), names_to = "Report Cat", values_to = "SD Diff")%>%
  ggplot( aes(x= `Report Cat`, y=`SD Diff`, fill= `Report Cat`)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    #theme_ipsum() +
    theme(
     
      plot.title = element_text(size=11),
      axis.title.x=element_blank(),
      axis.text.x=element_blank()
    ) +
     labs( y = "SD Diff",
            title = "School Content Reporting Cat. Variation by Achievement Level",
            x = "",
            caption = "NextGen HS Physics MCAS") +
    #xlab("")+
  facet_wrap(~`EM Perf Stat`)


# HSPhy_NextGen_SchoolSum%>%
#   select(`EM Perf Stat`, `MF%`, `EN%`, `WA%` )%>%
#   pivot_longer(c(2:4), names_to = "Report Cat", values_to = "% Points")%>%
#   ggplot( aes(x= `Report Cat`, y=`% Points`, fill= `Report Cat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     #theme_ipsum() +
#     theme(
#      
#       plot.title = element_text(size=11),
#       axis.title.x=element_blank(),
#       axis.text.x=element_blank()
#     ) +
#      labs( y = "SD Diff",
#             title = "School Content Reporting Cat. Performance by Achievement Level",
#             x = "",
#             caption = "NextGen HS Physics MCAS") +
#     #xlab("")+
#   facet_wrap(~`EM Perf Stat`)

# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`MF%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`MF Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# 
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`EN%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`EN Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`WA%`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")
# 
# HSPhy_NextGen_SchoolSum %>%
#   ggplot( aes(x= `EM Perf Stat`, y=`WA Diff SD`, fill= `EM Perf Stat`)) +
#     geom_boxplot() +
#     scale_fill_viridis(discrete = TRUE, alpha=0.6) +
#     geom_jitter(color="black", size=0.4, alpha=0.9) +
#     theme_ipsum() +
#     theme(
#       legend.position="none",
#       plot.title = element_text(size=11)
#     ) +
#     ggtitle("A boxplot with jitter") +
#     xlab("")

```

# References
